# instructions

---

One Billion Rows: Data Processing Challenge with Python

Introduction
This challenge was inspired by The One Billion Row Challenge, originally proposed for Java and adapted to Python by Luciano Galvão.

The goal of this project is to use Python to compare different methods of processing a massive data file containing 1 billion rows (~14GB) and calculating statistics (including aggregation and sorting, which are heavy operations) using Python. This task was replicated in three ways: using pandas, duckdb, and native Python.

The data file consists of temperature measurements from various weather stations, randomly generated by the create_measurements.py code and presented with one decimal precision. The dataset has two features: <string: station name> and <double: measurement>.

After generating the dataset, we developed a Python program capable of reading this file and calculating the minimum, average (rounded to one decimal place), and maximum temperature for each station, displaying the results in a table sorted by station name.

For each weather station:

It calculates the minimum temperature (min_temperature), the average temperature (mean_temperature), and the maximum temperature (max_temperature).
It returns this data in an organized manner, sorted by station.