# One Billion Row Challenge: Python Edition

This project is inspired by The One Billion Row Challenge, originally proposed for Java and later adapted to Python by Luciano Galv√£o.

### Dataset
The data consists of temperature measurements from various weather stations, randomly generated by the 'create_measurements.py' script. Each record has two attributes:
- Station Name: A string representing the weather station.
- Measurement: A floating-point value (with one decimal precision) representing the temperature.

```
Hamburg;12.0
Bulawayo;8.9
Palembang;38.8
St. John's;15.2
Cracow;12.6
Bridgetown;26.9
```
<br>

### Project Overview
The goal of this challenge is to compare different methods of processing a massive dataset (~14GB) containing 1 billion rows, using Python. The task involves calculating simple aggregations statistics (including aggregation and sorting) fir each weather stations:
- Minimum Temperature (min_temperature)
- Average Temperature (mean_temperature, rounded to one decimal place)
- Maximum Temperature (max_temperature)

and was replicated in five different ways:
- Pandas
- DuckDB
- Polars
- Polars with multiprocessing
- Native Python


### Setup - Instructions for installation and configuration in bash

1. Clone the git repository:
```bash
git clone https://github.com/kajinmo/data-engineering.git
cd data-engineering
```

2. Configure the correct Python version with `pyenv`:
```bash
pyenv install 3.12.3
pyenv local 3.12.3
```

3. Configure `poetry` for Python version 3.12.3 and activate the virtual environment:
```bash
poetry env use 3.12.3
poetry shell
```

4. Install project dependencies:
```bash
poetry install
```

5. Open vscode:
```bash
code .
```

6. run create_measurements.py
```shell
uv run create_measurements.py
```

7. run each code separately
```shell
using_duckdb.py
using_pandas.py
using_polars.py
using_polars_with_multiprocessing.py
using_python.py
```
---
<br>

### Results

| Approach | Time (sec) | Runtime increase vs. DuckDB
|---------|----------: | --------------:
| `duckdb` | 66.48 | 1.0
| `polars` | 227.22 | 3.4
| `pandas` | 446.21| 6.7
| `polars` with multiprocessing | 512.78 | 7.7
| `python` | 5863.08 | 88.2

- As can be seen, duckdb is the fastest, followed by polars.
- Pandas is the slowest by a factor of almost 7x when compared to DuckDB. 
- Surprisingly, using Polars with multiprocessing is worse than Pandas. This is because Polars is already highly optimized for parallel operations internally. Removing multiprocessing can reduce overhead and simplify the code (read more in https://docs.pola.rs/user-guide/misc/multiprocessing/).
- Pure Python is generally slower because isn't optimized for intensive loops, unlike libraries such as NumPy and Polars, which utilize vectorized operations and optimizations in C/Rust.